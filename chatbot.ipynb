{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3cf25bf-2be8-43eb-bc73-874887b3ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import annoy\n",
    "import codecs\n",
    "from random import sample \n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c465aa-565b-459a-81c1-4f8d7e554c2e",
   "metadata": {},
   "source": [
    "##### Осуществлён препроцессинг текста (как минимум удаление знаков препинания, приведение к нижнему регистру, стемминг/лемматизация)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce77641-ab20-4426-aa68-29f505b210cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_txt(line):\n",
    "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return spls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa92a10-8870-4790-b25c-0a9a8c66fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_csv('ProductsDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5a96e07-d559-4611-8455-1dd3c89605de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69085"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_list = []\n",
    "\n",
    "morpher = MorphAnalyzer()\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "product_df['title'].dropna().apply(lambda row: product_list.append(preprocess_txt(str(row))))\n",
    "w2v_pr_list= product_list \n",
    "product_df['descrirption'].dropna().apply(lambda row: product_list.append(preprocess_txt(str(row))))\n",
    "len(product_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72be5b6a-e054-4723-9564-368e5c288a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd36b97af6b3425e8041b37560b178d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = None\n",
    "written = False\n",
    "\n",
    "with codecs.open(\"prepared_answers.txt\",\"w\", \"utf-8\") as fout:\n",
    "    with codecs.open(\"Otvety.txt\", \"r\", \"utf-8\") as fin:\n",
    "        for line in tqdm(fin):\n",
    "            if line.startswith(\"---\"):\n",
    "                written = False\n",
    "                continue\n",
    "            if not written and question is not None:\n",
    "                fout.write(question.replace(\"\\t\", \" \").strip() + \"\\t\" + line.replace(\"\\t\", \" \"))\n",
    "                written = True\n",
    "                question = None\n",
    "                continue\n",
    "            if not written:\n",
    "                question = line.strip()\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3d0e4b-b6ce-4055-9bee-9762b596aa71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbedc9e0c23f4620850515b5638d692a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = []\n",
    "c = 0\n",
    "\n",
    "with codecs.open(\"Otvety.txt\", \"r\", \"utf-8\") as fin:\n",
    "    for line in tqdm(fin):\n",
    "        spls = preprocess_txt(line)\n",
    "        sentences.append(spls)\n",
    "        c += 1\n",
    "        if c > 500000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a55927-2a82-41df-9be8-925bc345fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [i for i in sentences if len(i) > 2]\n",
    "IR_df = pd.DataFrame({'data': product_list})\n",
    "concat_df = pd.DataFrame({'data': sample(sentences,70000)})\n",
    "IR_df['type'] = 'product'\n",
    "concat_df['type'] = 'other'\n",
    "IR_df = pd.concat([IR_df,concat_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d19b2-7f11-4ea9-a82e-2c5711617c18",
   "metadata": {},
   "source": [
    "##### Текст векторизирован любым из изученных способов (CountVectorizer, TfidfVectorizer, HashingVectorizer, Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e62493-a0ac-40e5-a845-135dde48f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, preprocessing\n",
    "\n",
    "\n",
    "x = []\n",
    "y = IR_df['type']\n",
    "\n",
    "for i in IR_df['data']:\n",
    "    x.append(\" \".join(i).strip())\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 3)).fit(x)\n",
    "x = tfidf.transform(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4753a4-db92-4ec0-b17c-c10f82bb3b49",
   "metadata": {},
   "source": [
    "##### Выборка разделена на обучающую и валидационную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a3f7718-c129-4421-bd2b-7bded197df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c7ddb-838b-4fe1-b3d0-bf26bf8dd751",
   "metadata": {},
   "source": [
    "### Обучить классификатор: продуктовый запрос vs. всё остальное (продуктовым можно считать запрос, который равен названию или описанию товара).\n",
    "\n",
    "##### Обучен классификатор с расчётом метрик на валидации (любое семейство алгоритмов, но предпочтительнее просто логистическая регрессия)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1f3d71-34d3-4e80-95cf-45b023fccc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "intent_recognition = LogisticRegression().fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d2a4a-ae45-4296-ae2a-a4fc5ceb7af2",
   "metadata": {},
   "source": [
    "##### Модель сохранена и при применении загружается из pkl-файла (или аналога)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c406773e-ae4f-4190-8040-54221f39e545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9564592200621189"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(intent_recognition, open('intent_recognitionl.pkl', 'wb'))\n",
    "intent_recognition = pickle.load(open('intent_recognitionl.pkl', 'rb'))\n",
    "y_pred = intent_recognition.predict(valid_x)\n",
    "accuracy_score(valid_y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2867f39-9aa9-4d4e-9bcd-e30c3103e2eb",
   "metadata": {},
   "source": [
    "### Реализована болталка\n",
    "\n",
    "Все вопросы из датасета свёрнуты Word2Vec в векторное представление.\n",
    "Построен индекс по вопросам.\n",
    "На запрос в болталку происходит поиск ближайшего вопроса и возвращается ответ на этот вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5badb140-7460-4436-87c6-d5829d8f8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "talk_w2v = Word2Vec(sentences=sentences, vector_size=100, min_count=1, window=5) \n",
    "talk_w2v.save(\"talk_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e4a8bb-3281-4ebc-be15-5e60ceb5ab37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4b7eba2fd34614aedcb8c2f0f1aaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talk_index = annoy.AnnoyIndex(100 ,'angular') \n",
    "\n",
    "talk_index_map = {}\n",
    "counter = 0\n",
    "\n",
    "with codecs.open(\"prepared_answers.txt\", \"r\", \"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        n_w2v = 0\n",
    "        spls = line.split(\"\\t\")\n",
    "        talk_index_map[counter] = spls[1]\n",
    "        question = preprocess_txt(spls[0])\n",
    "        vector = np.zeros(100)\n",
    "        for word in question:\n",
    "            if word in talk_w2v.wv:\n",
    "                vector += talk_w2v.wv[word]\n",
    "                n_w2v += 1\n",
    "        if n_w2v > 0:\n",
    "            vector = vector / n_w2v\n",
    "        talk_index.add_item(counter, vector)\n",
    "        counter += 1\n",
    "\n",
    "talk_index.build(10) #строим 10 деревьев\n",
    "talk_index.save('talk_speaker.ann')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d05e9f-681d-4e3c-9de7-4efc60665759",
   "metadata": {},
   "source": [
    "### Реализован поиск похожих товаров в контентной части бота\n",
    "\n",
    "##### Все названия товаров свёрнуты в векторное представление Word2Vec (предобученном или обученном на исходном датасете). )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55bb6c51-f634-423a-82f0-a19d30059444",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_w2v = Word2Vec(sentences=w2v_pr_list, vector_size=100, min_count=1, window=3) #кол-во измерений вектора, мин кол-во раз когда вектор попадется, \"окно\" сочетаний со словами вокрут взятого слова\n",
    "product_w2v.save(\"product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a9326f-060a-4276-a844-934c706b87c5",
   "metadata": {},
   "source": [
    "##### Построен индекс по названиям документов.Для товарных запросов реализован поиск в индексе (запрос также оборачивается Word2Vec, происходит проход в индекс).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "714e702f-2eb2-46bc-8c46-c42d4e55e7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_index = annoy.AnnoyIndex(100 ,'angular') \n",
    "\n",
    "product_index_map = {}\n",
    "\n",
    "for i in range(len(product_df)):\n",
    "    n_w2v = 0\n",
    "    product_index_map[i] = [product_df['product_id'][i],product_df['title'][i],product_df['descrirption'][i]]\n",
    "    product = preprocess_txt(product_df['title'][i])\n",
    "    vector = np.zeros(100)\n",
    "    for word in product:\n",
    "        if word in product_w2v.wv:\n",
    "            vector += product_w2v.wv[word]\n",
    "            n_w2v += 1\n",
    "    if n_w2v > 0:\n",
    "        vector = vector / n_w2v\n",
    "    product_index.add_item(i, vector)\n",
    "\n",
    "product_index.build(10) #строим 10 деревьев\n",
    "product_index.save('product_speaker.ann')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a729e07a-f949-43a4-997e-20d74b0a43ef",
   "metadata": {},
   "source": [
    "##### Добавить логику поиска похожих товаров по продуктовому запросу.\n",
    "##### Не был уверен как лучше оформить, так что просто засунул в print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e16d6df1-1681-48d6-a702-f17ccd705f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations(index,vector,index_map):\n",
    "    rec_indexes = index.get_nns_by_vector(vector, 5)\n",
    "    produc_info = index_map[rec_indexes[0]]\n",
    "    print('Ваш товар:\\n' + produc_info[1],\"\\nОписание:\",produc_info[2])\n",
    "    print('Похожие товары:')\n",
    "    for i in range(1,len(rec_indexes)):\n",
    "        produc_info = index_map[rec_indexes[i]]\n",
    "        print(i, produc_info[1],\"\\nОписание:\",produc_info[2])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c007e-4cbb-4a2e-a680-7b0afc5a95a3",
   "metadata": {},
   "source": [
    "##### Вся логика должна быть завёрнута в метод get_answer(). Ответ на продуктовый запрос должен иметь вид \"product_id title\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8f6e9db-5e02-4e39-93f7-4b17f4cc8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(message):\n",
    "    message = preprocess_txt(message)\n",
    "    intent_vec = tfidf.transform([\" \".join(message)])\n",
    "    if intent_recognition.predict(intent_vec) == 'product':\n",
    "        index_map = product_index_map\n",
    "        w2v = product_w2v.wv\n",
    "        index = product_index\n",
    "    else:\n",
    "        index_map = talk_index_map\n",
    "        w2v = talk_w2v.wv\n",
    "        index = talk_index\n",
    "    n_w2v = 0\n",
    "    vector = np.zeros(100)\n",
    "    for word in message:\n",
    "        if word in w2v:\n",
    "            vector += w2v[word]\n",
    "            n_w2v += 1\n",
    "    if n_w2v > 0:\n",
    "        vector = vector / n_w2v\n",
    "    answer_index = index.get_nns_by_vector(vector, 1)\n",
    "    if index_map != talk_index_map: \n",
    "        recommendations(index,vector,index_map)\n",
    "        return index_map[answer_index[0]][0]+' '+index_map[answer_index[0]][1]\n",
    "    else:\n",
    "        return index_map[answer_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcb41288-eded-497e-862a-9d78d09df3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваш товар:\n",
      "Джинсовая юбка \n",
      "Описание: Продаю фирменную и очень качественную джинсовую юбку Зара. Торг.\n",
      "Похожие товары:\n",
      "1 Юбка джинсовая \n",
      "Описание: Юбка джинсовая ,26 размер, стретч, длина -40 см .\n",
      "2 Джинсовые юбки \n",
      "Описание: 1)Мини-юбка джинсовая, Размер S. Талия-35,бедра-45,длина-41. Новая. В наличии 2шт. Цена-600р.\n",
      "2) Юбка befree, р-н 36. Б/у, состояние хорошее. Цена-400р.\n",
      "По договоренности возможна встреча на ст.метро Кантемировская.\n",
      "3 Юбка джинсовая \n",
      "Описание: Размер 40-42\n",
      "4 Юбка джинсовая \n",
      "Описание: В отличном состоянии!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5940d3a7bd36c023c4153162 Джинсовая юбка'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer('Джинсовая юбка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74aac2a9-9db7-4b4f-b76c-202393f5f40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хуже уже некуда. \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer('Как дела?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
