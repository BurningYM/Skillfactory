{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ae45cc-1166-4fda-81c4-0e655a2dc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import annoy\n",
    "import pickle\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models import FastText\n",
    "from tqdm import tqdm_notebook \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415fbb0c-9514-4e6d-80cf-eab7ef02ec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miron\\AppData\\Local\\Temp\\ipykernel_40048\\228411609.py:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  checks = pd.read_csv(\"check_data.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sale_date_date</th>\n",
       "      <th>contact_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>name</th>\n",
       "      <th>product_sub_category_id</th>\n",
       "      <th>product_category_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>1260627</td>\n",
       "      <td>1455.0</td>\n",
       "      <td>168308.0</td>\n",
       "      <td>(197312) Пакет-майка 25см х 45см</td>\n",
       "      <td>906.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>198287</td>\n",
       "      <td>279.0</td>\n",
       "      <td>134832.0</td>\n",
       "      <td>(62448) Перекись водорода р-р наружн. 3% фл.по...</td>\n",
       "      <td>404.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>2418385</td>\n",
       "      <td>848.0</td>\n",
       "      <td>101384.0</td>\n",
       "      <td>(72183) Салициловая кислота р-р спирт 2% фл 40...</td>\n",
       "      <td>404.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>1285774</td>\n",
       "      <td>1511.0</td>\n",
       "      <td>168570.0</td>\n",
       "      <td>(197309) Пакет 28см х 50см</td>\n",
       "      <td>906.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>1810323</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>168319.0</td>\n",
       "      <td>(197310) Пакет 30см х 60см</td>\n",
       "      <td>906.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1,00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sale_date_date contact_id  shop_id  product_id  \\\n",
       "0     2018-12-07    1260627   1455.0    168308.0   \n",
       "1     2018-12-07     198287    279.0    134832.0   \n",
       "2     2018-12-07    2418385    848.0    101384.0   \n",
       "3     2018-12-07    1285774   1511.0    168570.0   \n",
       "4     2018-12-07    1810323   1501.0    168319.0   \n",
       "\n",
       "                                                name  product_sub_category_id  \\\n",
       "0                   (197312) Пакет-майка 25см х 45см                    906.0   \n",
       "1  (62448) Перекись водорода р-р наружн. 3% фл.по...                    404.0   \n",
       "2  (72183) Салициловая кислота р-р спирт 2% фл 40...                    404.0   \n",
       "3                         (197309) Пакет 28см х 50см                    906.0   \n",
       "4                         (197310) Пакет 30см х 60см                    906.0   \n",
       "\n",
       "   product_category_id  brand_id quantity  \n",
       "0                205.0      -1.0     1,00  \n",
       "1                 93.0      -1.0     1,00  \n",
       "2                 93.0      -1.0     1,00  \n",
       "3                205.0      -1.0     1,00  \n",
       "4                205.0      -1.0     1,00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#загрузим исходные документы\n",
    "checks = pd.read_csv(\"check_data.csv\") \n",
    "with open(\"Product_dict.pkl\",\"rb\") as f:\n",
    "    product_dict = pickle.load(f)\n",
    "checks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4e834-2889-4382-b229-a21fbb072499",
   "metadata": {},
   "source": [
    "После небольшого анализа датасета можно выделить следующее:\n",
    "- Помимо даты продажи id товара и его описания, у нас так же есть:\n",
    "1. shop_id: идентефикатор магазина в котором происходила покупка\n",
    "2. product_sub_category_id/product_category_id/brand_id -подкатегория/категория товара и и id его бренда, все три в целом в теории могут помочь подобрать пользователю похожий товар. Все три страдают от того, что вместо правильных значений частов встречается -1\n",
    "3. quantity - колличество купленного товара, само по себе маловажно\n",
    "4. contact_id - возможно является идентефикатором покупателя, точно не является идентефикатором чека, так как покупки по одному id происходят в разные дни, так же не является id кассира/кассового аппарата так как на ~850 магазинов приходится 16464366 уникальных contact_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a318b12-d3c8-413c-a930-9c3274b0c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks.drop(['name'], axis=1, inplace=True)\n",
    "#Создадим признак по которому мы можем идентефицировать отдельный чек со всеми его товарами\n",
    "checks['sale_id'] = checks['sale_date_date'].apply(str) + \"_\" + checks['contact_id'].apply(str) + \"_\" + checks['shop_id'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a28511c-39cc-47f4-9940-b9bdae10febb",
   "metadata": {},
   "outputs": [],
   "source": [
    " #в попытке перевода на в дейттайм формат выдает ошибку в которой можно видеть что в sale_date_date есть не только даты, уберем не даты\n",
    "checks.dropna(inplace=True)\n",
    "checks = checks[checks['sale_date_date'].str.startswith(\"20\")]\n",
    "checks.reset_index(inplace=True, drop=True)\n",
    "checks['sale_date_date'] = pd.to_datetime(checks['sale_date_date'])\n",
    "checks.sort_values('sale_date_date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a3728c-0603-4dd7-8250-b28ffa886c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Поделим выборки на сплиты для валидации\n",
    "#Мы рискуем разделить часть куленных одновременно предметов не делая shuffle, но на результат это повлиять сильно не должно\n",
    "train, test = train_test_split(checks, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d041b0-c5b1-4a7a-b24b-0823b4edc38c",
   "metadata": {},
   "source": [
    "Относительно ленивое решение, лучше было сгрупировать датсет, а потом взять в разные колонки sale id и item_list(сгрупированные-id шники товаров), таким образом не было бы разделения одной покупки на две(часть в test часть в train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef48cfa-da51-418d-b257-5e1c72534cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reverse_product_dict = {j: k for k,j in product_dict.items()} # перевернем словарь для удобства поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b12ab8-2f12-4333-9cf1-7e2700ab7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "sw = get_stop_words(\"ru\")\n",
    "\n",
    "def preprocess(line):\n",
    "    spls = \"\".join(i for i in str(line).strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return spls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8dcaa85-a04d-448f-8b8d-fca5e70f8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [preprocess(str(i)[str(i).find(')')+1:]) for i in reverse_product_dict.keys()] #берем слова из словаря продуктов и предобрабатываем их, убирая число в начале\n",
    "sentences = [i for i in sentences if len(i) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7cbd3-3076-4a26-921e-d110fdf1c50b",
   "metadata": {},
   "source": [
    "#### Контекстные рекомендации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "552ae726-a854-434b-a195-f7e9ca9b928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_model= FastText(sentences=sentences, vector_size=20, min_count=1, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b68ba927-9ff1-4c32-a5f6-3a46446d87d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miron\\AppData\\Local\\Temp\\ipykernel_40048\\797071751.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for id in tqdm_notebook(product_dict.keys()):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b220308ea3364bb6aa71ef266c6f275f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30418 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_index = annoy.AnnoyIndex(20 ,'angular')\n",
    "index_dict = {}\n",
    "\n",
    "idc = 0\n",
    "\n",
    "for id in tqdm_notebook(product_dict.keys()):\n",
    "    n_ft = 0\n",
    "    index_dict[idc] = id\n",
    "    vector = np.zeros(20)\n",
    "    sen = product_dict[id]\n",
    "    for word in preprocess(str(sen)[str(sen).find(')')+1:]):\n",
    "        if word in FT_model.wv:\n",
    "            vector += FT_model.wv[word]\n",
    "            n_ft += 1\n",
    "    if n_ft > 0:\n",
    "        vector_ft = vector / n_ft\n",
    "    ft_index.add_item(idc, vector)\n",
    "    idc += 1\n",
    "\n",
    "# \n",
    "ft_index.build(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa38d1-31b1-402c-b389-8c1975a40c70",
   "metadata": {},
   "source": [
    "##### Изначально был сделан общий векторизатор который складывает векторы товаров и в корзине, но его результаты оставляли желать лучшего, так что было принятно решение выдавать рекомендации по каждому товару отдельно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "904c1cca-5e50-4598-8f9b-af4ecde919dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_vec(item): # для удобства чтения сделал отдельную функцию для создания вектора для одного товара\n",
    "    item = preprocess(str(item)[str(item).find(')')+1:])\n",
    "    vector = np.zeros(20)\n",
    "    n = 0\n",
    "    for word in item:\n",
    "        if word in FT_model.wv:\n",
    "            vector+= FT_model.wv[word]\n",
    "            n += 1\n",
    "    if n>0:\n",
    "        vector = vector/n\n",
    "    return vector\n",
    "\n",
    "def FT_recommend(id_list):\n",
    "    dist_dict = {}\n",
    "    vec_list = []\n",
    "    for id in id_list:\n",
    "        item_name = product_dict[id]\n",
    "        if item_name not in reverse_product_dict:\n",
    "            continue\n",
    "        vec_list.append(fasttext_vec(item_name)) #берем вектор товара и записываем его в список\n",
    "    for i in vec_list:\n",
    "        rec_items = []\n",
    "        rec_items = ft_index.get_nns_by_vector(i, 11) # берем 11 векторов(так как наш вектор уже в пространстве его будет выдавать первым\n",
    "        vec_id = rec_items[0] #сохраняем id нашего вектора в отдельную переменную\n",
    "        rec_items.pop(0) #убираем его из списка\n",
    "        for j in rec_items: #проходимся по всем ближайшим веторам \n",
    "            if ft_index.get_distance(vec_id,j) != 0: \n",
    "                dist_dict[ft_index.get_distance(vec_id,j)] = j #Ддобавляем в словарь дистанцию между искходным вектором+рекомендованным и id рекомендованного\n",
    "    dist_dict = sorted(dist_dict.items())[:10] #сортируем и берем 10 наиболее близких\n",
    "    return [index_dict[i[1]] for i in dist_dict]\n",
    "\n",
    "\"\"\"\n",
    "def FT_recommend(id_list): \n",
    "    list_vec = np.zeros(20)\n",
    "    l_len = len(id_list)\n",
    "    for id in id_list:\n",
    "        item_name = product_dict[id]\n",
    "        if item_name not in reverse_product_dict:\n",
    "            l_len -= 1\n",
    "            continue\n",
    "        list_vec+= fasttext_vec(item_name)\n",
    "    list_vec = list_vec/l_len\n",
    "    rec_items = ft_index.get_nns_by_vector(list_vec, 10+l_len)\n",
    "    id_to_return = [index_dict[i] for i in rec_items]\n",
    "    return [i for i in id_to_return if i not in id_list]\n",
    "\"\"\"          \n",
    "    \n",
    "def dict_check(list):\n",
    "    for i in list:\n",
    "        print(product_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e66944e4-890d-46b5-8a09-f464c30c67d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начальные товары:\n",
      "(197312) Пакет-майка 25см х 45см 906\n",
      "(65326) Кординорм табл. п.п.о. 5 мг №30 738\n",
      "(41123) Энап-H таб.10мг+25мг №20 738\n",
      "Рекомендации:\n",
      "(40894) Энап-HL таб.10мг+12,5мг №20 738\n",
      "(60023) Энап-HL таб.20мг+12,5мг №20 738\n",
      "(67751) Биол табл. п.п.о 5 мг №30 738\n",
      "(100304) Биол табл. п.п.о 2,5 мг №30 738\n",
      "(33282) Энап таб.2,5мг №20 738\n",
      "(30586) Нифекард ХЛ табл. п.п.о. 30 мг. №30 738\n",
      "(65312) Нипертен табл. п.п.о. 2.5 мг №30 738\n",
      "(200394) Пакет-майка 40см х 70см 906\n",
      "(109996) Симгал табл. п.п.о. 40 мг. №28 685\n",
      "(75050) Тулип табл. п.п.о. 40 мг. №30 685\n"
     ]
    }
   ],
   "source": [
    "print('Начальные товары:')\n",
    "dict_check(['168308','143681','101305'])\n",
    "print('Рекомендации:')\n",
    "dict_check(FT_recommend(['168308','143681','101305']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906609b-d34e-48e0-8d0c-cb42f400af5f",
   "metadata": {},
   "source": [
    "Получаем таким образом наиболие похожие товары по описанию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b54d832-5784-41e6-bc0e-aba7a4b10de2",
   "metadata": {},
   "source": [
    "### Word2Vec рекомендации по последовательностям товаров\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a28037ed-dca6-4ff6-a67e-378bf00bdecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2018-01-01_100232_312.0': [4743882], '2018-01-01_100608_312.0': [4725145, 4744187], '2018-01-01_1006676_332.0': [4724544, 4763456, 4740313], '2018-01-01_100886_597.0': [4739543, 4744612], '2018-01-01_101161_264.0': [4810506], '2018-01-01_1022374_43.0': [4763418, 4763502], '2018-01-01_1026344_160.0': [4740294, 4724962], '2018-01-01_1026759_160.0': [4767446, 4740359, 4767448, 4767445, 4767447], '2018-01-01_1031391_553.0': [6666931, 4826928, 6670536, 6679692], '2018-01-01_1034344_502.0': [4825663, 6679545], '2018-01-01_103741_264.0': [4712060], '2018-01-01_1038830_523.0': [4825645], '2018-01-01_103939_266.0': [4744101], '2018-01-01_1043280_332.0': [4704419, 4801655, 4690762], '2018-01-01_1043327_51.0': [4805708, 6509712], '2018-01-01_1044784_101.0': [4763505], '2018-01-01_1044919_281.0': [4763367], '2018-01-01_1045720_281.0': [4809756, 4809737, 4763451, 4704437], '2018-01-01_1045839_275.0': [4805723, 4763374], '2018-01-01_1050038_281.0': [4723618, 4763484], '2018-01-01_1051509_57.0': [4724660, 4740298, 4763413], '2018-01-01_1052586_278.0': [4724016, 4740375, 4705404, 4763390], '2018-01-01_105315_279.0': [4744635], '2018-01-01_1055707_836.0': [4722742, 4763375, 4704349, 4740320], '2018-01-01_1057681_835.0': [4767907, 4767905, 4767908, 4801045, 4767910, 4767911], '2018-01-01_1064195_607.0': [4699312, 4723784, 4704283, 4696864, 4800834, 4801719], '2018-01-01_1066104_532.0': [4825622], '2018-01-01_106842_331.0': [4743894], '2018-01-01_1068492_273.0': [4763453], '2018-01-01_1073223_585.0': [4825610], '2018-01-01_1074803_276.0': [4763471], '2018-01-01_108004_137.0': [4744073, 4739545, 4797678, 4719766], '2018-01-01_1083592_269.0': [4687217, 4807893, 4763428], '2018-01-01_108459_281.0': [4744112], '2018-01-01_1092793_331.0': [4724015, 4740363, 4763448], '2018-01-01_1097433_109.0': [4827014], '2018-01-01_109856_835.0': [4725148, 4744310], '2018-01-01_1102691_144.0': [4763409], '2018-01-01_1107135_145.0': [4704200], '2018-01-01_111037_602.0': [4804036, 4744422, 4799528], '2018-01-01_1112669_269.0': [4763341, 4740341, 4724414], '2018-01-01_1120094_278.0': [4763355], '2018-01-01_112296_268.0': [4744308, 4725334, 4712180], '2018-01-01_1127786_608.0': [4763449, 4740305], '2018-01-01_1132686_850.0': [4740391, 4723485], '2018-01-01_1133449_265.0': [4763465, 4740357], '2018-01-01_11396_266.0': [4744506], '2018-01-01_1139723_834.0': [4740402, 4809745], '2018-01-01_1140863_281.0': [4740415, 4801659], '2018-01-01_114301_332.0': [4744069, 4725495], '2018-01-01_1144047_603.0': [4763410], '2018-01-01_1144259_57.0': [4763343], '2018-01-01_1146916_57.0': [4796973, 4724659, 4701019, 4763381, 4704360, 4740370, 4801028], '2018-01-01_1148574_858.0': [4826249], '2018-01-01_1149295_278.0': [4690753, 4740303, 4687219, 4699306, 4724114, 4696859, 4698654, 4763447], '2018-01-01_1151143_331.0': [4740331, 4724320], '2018-01-01_1152820_275.0': [4805717, 4763463], '2018-01-01_1154412_144.0': [4740374], '2018-01-01_1154497_137.0': [6510756, 6510764], '2018-01-01_1154995_586.0': [6666999, 4825626, 6670691], '2018-01-01_1159571_317.0': [4740335, 4740398, 4704291, 4723941, 4763441], '2018-01-01_1160236_271.0': [4763401, 4740413], '2018-01-01_116062_598.0': [4810478, 4704399, 4701027, 4696844, 4744145, 4712051, 4687652, 6509487], '2018-01-01_1163901_137.0': [4740364, 4724871], '2018-01-01_1165638_835.0': [4723620, 4763427, 4723617], '2018-01-01_11665_317.0': [4737251, 4743801], '2018-01-01_1172615_275.0': [4690664, 4740339, 4723778], '2018-01-01_1182001_264.0': [4763508, 4740336], '2018-01-01_1185316_512.0': [6679543, 4825662], '2018-01-01_1186190_597.0': [4704223, 4690827, 4801690, 4723621, 4805715], '2018-01-01_11880_145.0': [4744496], '2018-01-01_11887_332.0': [4712151, 4704205, 4690799], '2018-01-01_1189335_270.0': [4763480, 4687222, 4740414, 4723940, 4704454, 4690679], '2018-01-01_1190459_312.0': [4722744, 4704197], '2018-01-01_1192322_243.0': [4763412], '2018-01-01_1192908_263.0': [4763444], '2018-01-01_119601_265.0': [4725236, 4744552], '2018-01-01_1197843_608.0': [4763368, 4687208, 4724319, 4740404, 4690817], '2018-01-01_1199269_277.0': [4763370], '2018-01-01_1200689_328.0': [4763461, 4722745], '2018-01-01_120497_849.0': [4801704], '2018-01-01_1205764_272.0': [4740337, 4763503], '2018-01-01_120678_160.0': [6510441], '2018-01-01_12100_137.0': [4712246, 4704204, 4687690, 4725291, 4690822], '2018-01-01_1212260_331.0': [4763361, 4687211, 4690792, 4704370], '2018-01-01_1212806_57.0': [4807484, 4704356, 4724658, 4763393, 4690690], '2018-01-01_1215227_278.0': [4740419, 4763377], '2018-01-01_1215647_278.0': [4763460], '2018-01-01_121565_263.0': [4725172, 4799555, 4744421], '2018-01-01_1218787_268.0': [4796968, 4724412, 4763500, 4807489, 4699319, 4704274, 4690661, 4698657, 4801682, 4696851, 4687210], '2018-01-01_122355_312.0': [4744052, 4725362], '2018-01-01_12257_110.0': [4815028], '2018-01-01_1226930_602.0': [4723622, 4809741, 4704178], '2018-01-01_1229946_317.0': [4704459, 4700967, 4763468, 4806922, 4699325], '2018-01-01_1231026_538.0': [4825667, 6679520], '2018-01-01_1235512_277.0': [4763467, 4704340, 4722908], '2018-01-01_123606_330.0': [4744067], '2018-01-01_1237275_869.0': [6670585, 4826241, 6679788], '2018-01-01_1237516_332.0': [4687221, 4696861, 4700958, 4763450, 4690756, 4704436], '2018-01-01_1238100_279.0': [4763437], ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ранее мы подготовили для групировки по корзине признак sale_id, осталось лишь сгрупировать по этому id наш датасет и скормить данные обученой w2v модели(которую еще надо обучить)\n",
    "train['product_id'] = train['product_id'].apply(int).apply(str)\n",
    "check_group = train.groupby('sale_id')\n",
    "check_group.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4332917-ea02-4ce9-b8a2-27b9be9aefb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miron\\AppData\\Local\\Temp\\ipykernel_40048\\70507501.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for group in tqdm_notebook(check_group.groups):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2935efc4874b12ad0f14b45eeb76e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5457442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['62087', '120171', '69028']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Перенесем в спиок комбинации продуктов для обучения word2vec модели, брать будем только чеки в которых 3 и более предметов\n",
    "sentences = []\n",
    "for group in tqdm_notebook(check_group.groups):\n",
    "    products = check_group.get_group(group)['product_id'].values\n",
    "    if len(products) < 3:\n",
    "        continue\n",
    "    sentences.append(list(products))\n",
    "\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71277efa-baea-4f42-8b01-376cb92231c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_model = Word2Vec(sentences, vector_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1190a00-74ac-4053-9a14-41f8197656b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V_prediction(item_list):\n",
    "    vec = np.zeros(10)\n",
    "    len_list= len(item_list)\n",
    "    for item in item_list:\n",
    "        if item not in product_dict:\n",
    "            len_list -=1\n",
    "            continue\n",
    "        vec += W2V_model.wv[item]\n",
    "    return [i[0] for i in W2V_model.wv.similar_by_vector(vec / len_list, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07af5424-e818-4262-bec9-b95eebc1c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начальные товары:\n",
      "(197312) Пакет-майка 25см х 45см 906\n",
      "(65326) Кординорм табл. п.п.о. 5 мг №30 738\n",
      "(41123) Энап-H таб.10мг+25мг №20 738\n",
      "Рекомендации:\n",
      "(38101) Капотен тб 25мг N56 738\n",
      "(43433) Капотен тб 25мг N40 738\n",
      "(40897) Энап-HL таб.10мг+12,5мг №20 738\n",
      "(78752) Атенолол Никомед табл. п.п.о. 50 мг. №30 738\n",
      "(38772) Коринфар таб. п.о. 10мг фл №50 738\n",
      "(110948) Энап таб.20мг №60 738\n",
      "(15924) Кордипин ретард табл. п.о. ретард 20 мг. №30 -1\n",
      "(39158) Коринфар таб. п.о. 10мг фл №100 738\n",
      "(64563) Эналаприл Гексал таб.10мг №20 738\n",
      "(45741) Капозид тб 50мг N28 738\n"
     ]
    }
   ],
   "source": [
    "print('Начальные товары:')\n",
    "dict_check(['168308','143681','101305'])\n",
    "print('Рекомендации:')\n",
    "dict_check(W2V_prediction(['168308','143681','101305']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba11a5b-b86f-4848-ba6f-5c049e822c15",
   "metadata": {},
   "source": [
    "В отличии от контекстных рекомендаций, вывод w2v по последовательностям не так очевиден, по крайней мере для не фармацевтов. Что мы можем сделать для его проверки это взять заранее отложенный тестовый датасет, сгруппировать его так же как сгруппировали тренировочный, взять в нем чеки из трех и более товаров и перенести один или несколько товаров в отдельную колонку для того чтобы посмотреть порекомендует ли нам этот товар наша модель. После чего провести тестовую выборку по нашей функции и посчитать точность например через Precision@N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ec4a6-a524-42ce-ab4c-cdf3b35cc536",
   "metadata": {},
   "source": [
    "### Двухстадийная рекомендательная система \n",
    "#### Выбор кандидатов\n",
    "Возьмем 7 w2v и 3 FT, обусловлено это тем, что качество, как минимум на первый взгляд, у w2v модели выше, FT уделяет слишком большое внимание граммовкам и остальному дополнительному тексту по типу \"аб.10мг+12,5мг №20 738\". Cсоотношени может изменится при более глубоком тестировании, но для начала возьмем так.е\n",
    "#### Ранжирование\n",
    "Существует огромное множестов подходов ранжирования. Учитывая что мы ищем рекомендации по товарам в корзине и тому что ранжируем мы две модели, модель которая ищет наиболее близкий товар по названию, и модель которая предсказывает товар по группам товаров в корзине.(своебразные версии \"похожие товары\" и \"Часто покупаемые вместе\")В голову приходятя два варианта ранжирования.\n",
    "1. Более сложное - берем все записанные группы товаров в датаесете, вынимает случайный товар в отдельный столбец и обучаем на этом модель. Которая впоследствии будет предсказывать \"уверенность\" в наших ответа, по ней и ранжируемх.\n",
    "2. Менее сложное - в качестве системы скоринга берем схожесть из w2v и 1 - дистанция*2(в качестве доп. веса) между вещью в корзине и предсказанной вещью из FT, считаем это за оценки и ранжируем по ним.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b678e9e-a8bb-4f83-8f50-f4f0b465d692",
   "metadata": {},
   "source": [
    "Пойдем по пути наименьшего сопротивление и выберем второй вариант"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c6463-d0f6-4378-b938-50292195e61a",
   "metadata": {},
   "source": [
    "Оставим предыдущие функции не тронутыми и переделаем их под наши нужды отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f49b363a-4582-480c-b9a5-e6b2fe755b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FT_for_range(id_list):\n",
    "    dist_dict = {}\n",
    "    vec_list = []\n",
    "    for id in id_list:\n",
    "        item_name = product_dict[id]\n",
    "        if item_name not in reverse_product_dict:\n",
    "            continue\n",
    "        vec_list.append(fasttext_vec(item_name)) #берем вектор товара и записываем его в список\n",
    "    for i in vec_list:\n",
    "        rec_items = []\n",
    "        rec_items = ft_index.get_nns_by_vector(i, 11) # берем 11 векторов(так как наш вектор уже в пространстве его будет выдавать первым\n",
    "        vec_id = rec_items[0] #сохраняем id нашего вектора в отдельную переменную\n",
    "        rec_items.pop(0) #убираем его из списка\n",
    "        for j in rec_items: #проходимся по всем ближайшим веторам \n",
    "            if ft_index.get_distance(vec_id,j) != 0: \n",
    "                dist_dict[ft_index.get_distance(vec_id,j)] = j #Ддобавляем в словарь дистанцию между искходным вектором+рекомендованным и id рекомендованного\n",
    "    dist_dict = dict(sorted(dist_dict.items())[:3]) #сортируем и берем 3 наиболее близких\n",
    "    return dist_dict\n",
    "\n",
    "def W2V_for_range(item_list):\n",
    "    vec = np.zeros(10)\n",
    "    len_list= len(item_list)\n",
    "    for item in item_list:\n",
    "        if item not in product_dict:\n",
    "            len_list -=1\n",
    "            continue\n",
    "        vec += W2V_model.wv[item]\n",
    "    return W2V_model.wv.similar_by_vector(vec / len_list, 7)\n",
    "\n",
    "def ranging(item_list):\n",
    "    combined_dict = dict(W2V_for_range(item_list)) #берем 7 наиболее вероятных предсказаний из w2v\n",
    "    FT_dict = FT_for_range(item_list) #берем 3 из FT\n",
    "    FT_dict = {index_dict[j]: 1-k*2 for k,j in FT_dict.items()} # погодняем ft под w2v меняем индекс на id товара, и \"оцениваем\" расстояние вектора\n",
    "    combined_dict.update(FT_dict)\n",
    "    combined_dict = {j: k for k,j in combined_dict.items()}\n",
    "    return [i[1] for i in sorted(combined_dict.items(), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d5251b4b-8973-4093-9311-1f437fe8728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начальные товары:\n",
      "(197312) Пакет-майка 25см х 45см 906\n",
      "(65326) Кординорм табл. п.п.о. 5 мг №30 738\n",
      "(41123) Энап-H таб.10мг+25мг №20 738\n",
      "Рекомендации:\n",
      "(40894) Энап-HL таб.10мг+12,5мг №20 738\n",
      "(60023) Энап-HL таб.20мг+12,5мг №20 738\n",
      "(67751) Биол табл. п.п.о 5 мг №30 738\n",
      "(38101) Капотен тб 25мг N56 738\n",
      "(43433) Капотен тб 25мг N40 738\n",
      "(40897) Энап-HL таб.10мг+12,5мг №20 738\n",
      "(78752) Атенолол Никомед табл. п.п.о. 50 мг. №30 738\n",
      "(38772) Коринфар таб. п.о. 10мг фл №50 738\n",
      "(110948) Энап таб.20мг №60 738\n",
      "(15924) Кордипин ретард табл. п.о. ретард 20 мг. №30 -1\n"
     ]
    }
   ],
   "source": [
    "print('Начальные товары:')\n",
    "dict_check(['168308','143681','101305'])\n",
    "print('Рекомендации:')\n",
    "dict_check(ranging(['168308','143681','101305']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d420633b-7a00-47c3-ad52-934aa532d131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['62144',\n",
       " '126081',\n",
       " '124362',\n",
       " '102814',\n",
       " '115626',\n",
       " '142899',\n",
       " '150856',\n",
       " '124198',\n",
       " '93397',\n",
       " '59086']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranging(['168308','143681','101305'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
